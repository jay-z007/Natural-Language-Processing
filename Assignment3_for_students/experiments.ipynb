{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "from DependencyTree import DependencyTree\n",
    "from ParsingSystem import ParsingSystem\n",
    "from Configuration import Configuration\n",
    "import Config\n",
    "import Util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDictionaries(sents, trees):\n",
    "    word = []\n",
    "    pos = []\n",
    "    label = []\n",
    "    for s in sents:\n",
    "        for token in s:\n",
    "            word.append(token['word'])\n",
    "            pos.append(token['POS'])\n",
    "\n",
    "    rootLabel = None\n",
    "    for tree in trees:\n",
    "        for k in range(1, tree.n + 1):\n",
    "            if tree.getHead(k) == 0:\n",
    "                rootLabel = tree.getLabel(k)\n",
    "            else:\n",
    "                label.append(tree.getLabel(k))\n",
    "\n",
    "    if rootLabel in label:\n",
    "        label.remove(rootLabel)\n",
    "\n",
    "    index = 0\n",
    "    wordCount = [Config.UNKNOWN, Config.NULL, Config.ROOT]\n",
    "    wordCount.extend(collections.Counter(word))\n",
    "    for word in wordCount:\n",
    "        wordDict[word] = index\n",
    "        index += 1\n",
    "\n",
    "    posCount = [Config.UNKNOWN, Config.NULL, Config.ROOT]\n",
    "    posCount.extend(collections.Counter(pos))\n",
    "    for pos in posCount:\n",
    "        posDict[pos] = index\n",
    "        index += 1\n",
    "\n",
    "    labelCount = [Config.NULL, rootLabel]\n",
    "    labelCount.extend(collections.Counter(label))\n",
    "    for label in labelCount:\n",
    "        labelDict[label] = index\n",
    "        index += 1\n",
    "\n",
    "    return wordDict, posDict, labelDict\n",
    "\n",
    "\n",
    "def getWordID(s):\n",
    "    if s in wordDict:\n",
    "        return wordDict[s]\n",
    "    else:\n",
    "        return wordDict[Config.UNKNOWN]\n",
    "\n",
    "\n",
    "def getPosID(s):\n",
    "    if s in posDict:\n",
    "        return posDict[s]\n",
    "    else:\n",
    "        return posDict[Config.UNKNOWN]\n",
    "\n",
    "\n",
    "def getLabelID(s):\n",
    "    if s in labelDict:\n",
    "        return labelDict[s]\n",
    "    else:\n",
    "        return labelDict[Config.UNKNOWN]\n",
    "\n",
    "\n",
    "def getFeatures(c):\n",
    "\n",
    "    \"\"\"\n",
    "    =================================================================\n",
    "\n",
    "    Implement feature extraction described in\n",
    "    \"A Fast and Accurate Dependency Parser using Neural Networks\"(2014)\n",
    "\n",
    "    =================================================================\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    ## Token indices\n",
    "    s1 = c.getStack(0)\n",
    "    s2 = c.getStack(1)\n",
    "    s3 = c.getStack(2)\n",
    "    b1 = c.getBuffer(0)\n",
    "    b2 = c.getBuffer(1)\n",
    "    b3 = c.getBuffer(2)\n",
    "\n",
    "    lc1_s1 = c.getLeftChild(s1, 1)\n",
    "    rc1_s1 = c.getRightChild(s1, 1)\n",
    "    lc2_s1 = c.getLeftChild(s1, 2)\n",
    "    rc2_s1 = c.getRightChild(s1, 2)\n",
    "\n",
    "    lc1_s2 = c.getLeftChild(s2, 1)\n",
    "    rc1_s2 = c.getRightChild(s2, 1)\n",
    "    lc2_s2 = c.getLeftChild(s2, 2)\n",
    "    rc2_s2 = c.getRightChild(s2, 2)\n",
    "\n",
    "    lc1_lc1_s1 = c.getLeftChild(lc1_s1, 1)\n",
    "    rc1_rc1_s1 = c.getRightChild(rc1_s1, 1)\n",
    "\n",
    "    lc1_lc1_s2 = c.getLeftChild(lc1_s2, 1)\n",
    "    rc1_rc1_s2 = c.getRightChild(rc1_s2, 1)\n",
    "\n",
    "    ## Word IDs\n",
    "    word_ids = [\n",
    "        getWordID(c.getWord(s1)),\n",
    "        getWordID(c.getWord(s2)),\n",
    "        getWordID(c.getWord(s3)),\n",
    "        getWordID(c.getWord(b1)),\n",
    "        getWordID(c.getWord(b2)),\n",
    "        getWordID(c.getWord(b3)),\n",
    "        getWordID(c.getWord(lc1_s1)),\n",
    "        getWordID(c.getWord(rc1_s1)),\n",
    "        getWordID(c.getWord(lc2_s1)),\n",
    "        getWordID(c.getWord(rc2_s1)),\n",
    "        getWordID(c.getWord(lc1_s2)),\n",
    "        getWordID(c.getWord(rc1_s2)),\n",
    "        getWordID(c.getWord(lc2_s2)),\n",
    "        getWordID(c.getWord(rc2_s2)),\n",
    "        getWordID(c.getWord(lc1_lc1_s1)),\n",
    "        getWordID(c.getWord(rc1_rc1_s1)),\n",
    "        getWordID(c.getWord(lc1_lc1_s2)),\n",
    "        getWordID(c.getWord(rc1_rc1_s2))\n",
    "    ]\n",
    "    \n",
    "    ## POS IDs\n",
    "    pos_ids = [\n",
    "        getPosID(c.getPOS(s1)),\n",
    "        getPosID(c.getPOS(s2)),\n",
    "        getPosID(c.getPOS(s3)),\n",
    "        getPosID(c.getPOS(b1)),\n",
    "        getPosID(c.getPOS(b2)),\n",
    "        getPosID(c.getPOS(b3)),\n",
    "        getPosID(c.getPOS(lc1_s1)),\n",
    "        getPosID(c.getPOS(rc1_s1)),\n",
    "        getPosID(c.getPOS(lc2_s1)),\n",
    "        getPosID(c.getPOS(rc2_s1)),\n",
    "        getPosID(c.getPOS(lc1_s2)),\n",
    "        getPosID(c.getPOS(rc1_s2)),\n",
    "        getPosID(c.getPOS(lc2_s2)),\n",
    "        getPosID(c.getPOS(rc2_s2)),\n",
    "        getPosID(c.getPOS(lc1_lc1_s1)),\n",
    "        getPosID(c.getPOS(rc1_rc1_s1)),\n",
    "        getPosID(c.getPOS(lc1_lc1_s2)),\n",
    "        getPosID(c.getPOS(rc1_rc1_s2))\n",
    "    ]\n",
    "\n",
    "    ## Labels IDs\n",
    "    label_ids = [\n",
    "        getLabelID(c.getLabel(lc1_s1)),\n",
    "        getLabelID(c.getLabel(rc1_s1)),\n",
    "        getLabelID(c.getLabel(lc2_s1)),\n",
    "        getLabelID(c.getLabel(rc2_s1)),\n",
    "        getLabelID(c.getLabel(lc1_s2)),\n",
    "        getLabelID(c.getLabel(rc1_s2)),\n",
    "        getLabelID(c.getLabel(lc2_s2)),\n",
    "        getLabelID(c.getLabel(rc2_s2)),\n",
    "        getLabelID(c.getLabel(lc1_lc1_s1)),\n",
    "        getLabelID(c.getLabel(rc1_rc1_s1)),\n",
    "        getLabelID(c.getLabel(lc1_lc1_s2)),\n",
    "        getLabelID(c.getLabel(rc1_rc1_s2))\n",
    "    ]\n",
    "\n",
    "    features.extend(word_ids)\n",
    "    features.extend(pos_ids)\n",
    "    features.extend(label_ids)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def genTrainExamples(sents, trees):\n",
    "    numTrans = parsing_system.numTransitions()\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    pbar = ProgressBar()\n",
    "    for i in pbar(range(len(sents))):\n",
    "        if trees[i].isProjective():\n",
    "            c = parsing_system.initialConfiguration(sents[i])\n",
    "\n",
    "            while not parsing_system.isTerminal(c):\n",
    "                oracle = parsing_system.getOracle(c, trees[i])\n",
    "                feat = getFeatures(c)\n",
    "                label = []\n",
    "                for j in range(numTrans):\n",
    "                    t = parsing_system.transitions[j]\n",
    "                    if t == oracle:\n",
    "                        label.append(1.)\n",
    "                    elif parsing_system.canApply(c, t):\n",
    "                        label.append(0.)\n",
    "                    else:\n",
    "                        label.append(-1.)\n",
    "\n",
    "                if 1.0 not in label:\n",
    "                    print i, label\n",
    "                features.append(feat)\n",
    "                labels.append(label)\n",
    "                c = parsing_system.apply(c, oracle)\n",
    "\n",
    "            # if c.tree.equal(trees[i]):\n",
    "            #     print(\"Apply function is working Fine\")\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def load_embeddings(filename, wordDict, posDict, labelDict):\n",
    "    dictionary, word_embeds = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "    word_len = len(wordDict)\n",
    "    pos_len = len(posDict)\n",
    "    dep_len = len(labelDict)\n",
    "    \n",
    "    embedding_array = np.zeros((len(wordDict) + len(posDict) + len(labelDict), Config.embedding_size))\n",
    "    knownWords = wordDict.keys()    \n",
    "    foundEmbed = 0\n",
    "    \n",
    "    for i in range(len(embedding_array)):\n",
    "        index = -1\n",
    "        if i < len(knownWords):\n",
    "            w = knownWords[i]\n",
    "            if w in dictionary:\n",
    "                index = dictionary[w]\n",
    "            elif w.lower() in dictionary:\n",
    "                index = dictionary[w.lower()]\n",
    "        if index >= 0:\n",
    "            foundEmbed += 1\n",
    "            embedding_array[i] = word_embeds[index]\n",
    "        else:\n",
    "#             embedding_array[i] = np.random.rand(Config.embedding_size) * 0.02 - 0.01\n",
    "\n",
    "            embedding_array[i][i%Config.embedding_size] = 1\n",
    "            \n",
    "    print \"Found embeddings: \", foundEmbed, \"/\", len(knownWords)\n",
    "\n",
    "    return embedding_array\n",
    "\n",
    "\n",
    "wordDict = {}\n",
    "posDict = {}\n",
    "labelDict = {}\n",
    "parsing_system = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings:  30160 / 44392\n"
     ]
    }
   ],
   "source": [
    "trainSents, trainTrees = Util.loadConll('train.conll')\n",
    "devSents, devTrees = Util.loadConll('dev.conll')\n",
    "testSents, _ = Util.loadConll('test.conll')\n",
    "genDictionaries(trainSents, trainTrees)\n",
    "\n",
    "embedding_filename = 'word2vec.model'\n",
    "\n",
    "embedding_array = load_embeddings(embedding_filename, wordDict, posDict, labelDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "Generating Traning Examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% |####################################################################### |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r\n"
     ]
    }
   ],
   "source": [
    "labelInfo = []\n",
    "for idx in np.argsort(labelDict.values()):\n",
    "    labelInfo.append(labelDict.keys()[idx])\n",
    "parsing_system = ParsingSystem(labelInfo[1:])\n",
    "print parsing_system.rootLabel\n",
    "\n",
    "print \"Generating Traning Examples\"\n",
    "trainFeats, trainLabels = genTrainExamples(trainSents, trainTrees)\n",
    "print \"Done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DependencyParserModel(object):\n",
    "\n",
    "    def __init__(self, graph, embedding_array, Config):\n",
    "\n",
    "        self.build_graph(graph, embedding_array, Config)\n",
    "\n",
    "    def build_graph(self, graph, embedding_array, Config):\n",
    "        \"\"\"\n",
    "\n",
    "        :param graph:\n",
    "        :param embedding_array:\n",
    "        :param Config:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        with graph.as_default():\n",
    "\n",
    "            \"\"\"\n",
    "            ===================================================================\n",
    "\n",
    "            Define the computational graph with necessary variables.\n",
    "            \n",
    "            1) You may need placeholders of:\n",
    "                - Many parameters are defined at Config: batch_size, n_Tokens, etc\n",
    "                - # of transitions can be get by calling parsing_system.numTransitions()\n",
    "                \n",
    "            self.train_inputs = \n",
    "            self.train_labels = \n",
    "            self.test_inputs =\n",
    "            ...\n",
    "            \n",
    "                \n",
    "            2) Call forward_pass and get predictions\n",
    "            \n",
    "            ...\n",
    "            self.prediction = self.forward_pass(embed, weights_input, biases_input, weights_output)\n",
    "\n",
    "\n",
    "            3) Implement the loss function described in the paper\n",
    "             - lambda is defined at Config.lam\n",
    "            \n",
    "            ...\n",
    "            self.loss =\n",
    "            \n",
    "            ===================================================================\n",
    "            \"\"\"\n",
    "            self.embeddings = tf.Variable(embedding_array, dtype=tf.float32, trainable=False)\n",
    "            \n",
    "            n_transitions = parsing_system.numTransitions()\n",
    "\n",
    "            self.train_inputs = tf.placeholder(tf.int32, shape=(Config.batch_size, Config.n_Tokens))#, name='Train Inputs')\n",
    "            self.train_labels = tf.placeholder(tf.float32, shape=(Config.batch_size, n_transitions))#, name=\"Train Labels\")\n",
    "            self.test_inputs = tf.placeholder(tf.int32, shape=(48,))#, name='Test Inputs')\n",
    "\n",
    "            ## lookup from embedding array\n",
    "            train_embed = tf.nn.embedding_lookup(self.embeddings, self.train_inputs)\n",
    "            train_embed = tf.reshape(train_embed, [Config.batch_size, -1])\n",
    "\n",
    "#             ## generate weights and biases\n",
    "#             weights_input = tf.Variable(tf.random.truncated_normal(shape=(Config.embedding_size*Config.n_Tokens , Config.hidden_size), stddev=0.1))\n",
    "#             biases_input = tf.Variable(tf.random.truncated_normal(shape=(1, Config.hidden_size), stddev=0.1))\n",
    "#             weights_output = tf.Variable(tf.random.truncated_normal(shape=(Config.hidden_size, n_transitions), stddev=0.1))\n",
    "\n",
    "#             Generalized\n",
    "            shapes = [Config.embedding_size*Config.n_Tokens] + Config.hidden_size + [n_transitions]\n",
    "            weights = [tf.Variable(tf.random.truncated_normal((shape1, shape2), stddev=0.1)) for shape1, shape2 in zip(shapes, shapes[1:])]\n",
    "            biases = [tf.Variable(tf.random.truncated_normal((1, shape), stddev=0.1)) for shape in Config.hidden_size]\n",
    "\n",
    "#             Parallel\n",
    "#             weights_word = tf.Variable(tf.random.truncated_normal((Config.embedding_size*18, Config.hidden_size[0]), stddev=0.1))\n",
    "#             weights_pos = tf.Variable(tf.random.truncated_normal((Config.embedding_size*18, Config.hidden_size[0]), stddev=0.1))\n",
    "#             weights_dep = tf.Variable(tf.random.truncated_normal((Config.embedding_size*12, Config.hidden_size[0]), stddev=0.1))\n",
    "#             weights_output = tf.Variable(tf.random.truncated_normal((Config.hidden_size[0]*3, n_transitions), stddev=0.1))\n",
    "\n",
    "#             weights = [weights_word, weights_pos, weights_dep, weights_output]\n",
    "#             biases = [tf.Variable(tf.random.truncated_normal((1, Config.hidden_size[0]), stddev=0.1))]*3\n",
    "    \n",
    "            ## Generate mask for ignoring the transitions with -1 label\n",
    "            mask = tf.cast(self.train_labels >= 0, tf.float32)\n",
    "\n",
    "            ## forward pass\n",
    "#             self.prediction = self.forward_pass(train_embed, weights_input, biases_input, weights_output)\n",
    "            self.prediction = self.forward_pass(train_embed, weights, biases)\n",
    "            \n",
    "            pred = tf.math.multiply(self.prediction, mask)\n",
    "            labels = tf.argmax(self.train_labels, axis=1)\n",
    "            \n",
    "            ## loss\n",
    "#             regularization = Config.lam * (tf.nn.l2_loss(weights_input) + tf.nn.l2_loss(weights_output) + tf.nn.l2_loss(biases_input)) \n",
    "#             cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=labels))\n",
    "            regularization = Config.lam * sum(tf.nn.l2_loss(w) for w in weights+biases)\n",
    "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(logits=pred, labels=labels)\n",
    "            self.loss = cross_entropy + regularization\n",
    "\n",
    "#             optimizer = tf.train.GradientDescentOptimizer(Config.learning_rate)\n",
    "#             optimizer = tf.train.AdagradOptimizer(Config.learning_rate)\n",
    "            optimizer = tf.train.AdamOptimizer(Config.learning_rate)\n",
    "            grads = optimizer.compute_gradients(self.loss)\n",
    "#             clipped_grads = [(tf.clip_by_norm(grad, 5), var) for grad, var in grads]\n",
    "#             self.app = optimizer.apply_gradients(clipped_grads)\n",
    "            self.app = optimizer.apply_gradients(grads)\n",
    "\n",
    "            # For test data, we only need to get its prediction\n",
    "            test_embed = tf.nn.embedding_lookup(self.embeddings, self.test_inputs)\n",
    "            test_embed = tf.reshape(test_embed, [1, -1])\n",
    "#             self.test_pred = self.forward_pass(test_embed, weights_input, biases_input, weights_output)\n",
    "            self.test_pred = self.forward_pass(test_embed, weights, biases)\n",
    "\n",
    "            # intializer\n",
    "            self.init = tf.global_variables_initializer()\n",
    "\n",
    "    def train(self, sess, num_steps):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sess:\n",
    "        :param num_steps:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.init.run()\n",
    "        print \"Initailized\"\n",
    "\n",
    "        average_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            start = (step * Config.batch_size) % len(trainFeats)\n",
    "            end = ((step + 1) * Config.batch_size) % len(trainFeats)\n",
    "            if end < start:\n",
    "                start -= end\n",
    "                end = len(trainFeats)\n",
    "            batch_inputs, batch_labels = trainFeats[start:end], trainLabels[start:end]\n",
    "\n",
    "            feed_dict = {self.train_inputs: batch_inputs, self.train_labels: batch_labels}\n",
    "\n",
    "            _, loss_val = sess.run([self.app, self.loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % Config.display_step == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= Config.display_step\n",
    "                print \"Average loss at step \", step, \": \", average_loss\n",
    "                average_loss = 0\n",
    "            if step % Config.validation_step == 0 and step != 0:\n",
    "                print \"\\nTesting on dev set at step \", step\n",
    "                predTrees = []\n",
    "                for sent in devSents:\n",
    "                    numTrans = parsing_system.numTransitions()\n",
    "\n",
    "                    c = parsing_system.initialConfiguration(sent)\n",
    "                    while not parsing_system.isTerminal(c):\n",
    "                        feat = getFeatures(c)\n",
    "                        pred = sess.run(self.test_pred, feed_dict={self.test_inputs: feat})\n",
    "\n",
    "                        optScore = -float('inf')\n",
    "                        optTrans = \"\"\n",
    "\n",
    "                        for j in range(numTrans):\n",
    "                            if pred[0, j] > optScore and parsing_system.canApply(c, parsing_system.transitions[j]):\n",
    "                                optScore = pred[0, j]\n",
    "                                optTrans = parsing_system.transitions[j]\n",
    "\n",
    "                        c = parsing_system.apply(c, optTrans)\n",
    "\n",
    "                    predTrees.append(c.tree)\n",
    "                result = parsing_system.evaluate(devSents, predTrees, devTrees)\n",
    "                print result\n",
    "\n",
    "        print \"Train Finished.\"\n",
    "\n",
    "    def evaluate(self, sess, testSents):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sess:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        print \"Starting to predict on test set\"\n",
    "        predTrees = []\n",
    "        for sent in testSents:\n",
    "            numTrans = parsing_system.numTransitions()\n",
    "\n",
    "            c = parsing_system.initialConfiguration(sent)\n",
    "            while not parsing_system.isTerminal(c):\n",
    "                # feat = getFeatureArray(c)\n",
    "                feat = getFeatures(c)\n",
    "                pred = sess.run(self.test_pred, feed_dict={self.test_inputs: feat})\n",
    "\n",
    "                optScore = -float('inf')\n",
    "                optTrans = \"\"\n",
    "\n",
    "                for j in range(numTrans):\n",
    "                    if pred[0, j] > optScore and parsing_system.canApply(c, parsing_system.transitions[j]):\n",
    "                        optScore = pred[0, j]\n",
    "                        optTrans = parsing_system.transitions[j]\n",
    "\n",
    "                c = parsing_system.apply(c, optTrans)\n",
    "\n",
    "            predTrees.append(c.tree)\n",
    "        print \"Saved the test results.\"\n",
    "        Util.writeConll('result_test.conll', testSents, predTrees)\n",
    "\n",
    "\n",
    "    def forward_pass(self, embed, weights, biases):\n",
    "#     def forward_pass(self, embed, weights_input, biases_input, weights_output):\n",
    "        \"\"\"\n",
    "\n",
    "        :param embed: batch_size, feature_size*embedding_size\n",
    "        :param weights: feature_size*embedding_size, hidden_size\n",
    "        :param biases: hidden_size\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        =======================================================\n",
    "\n",
    "        Implement the forwrad pass described in\n",
    "        \"A Fast and Accurate Dependency Parser using Neural Networks\"(2014)\n",
    "\n",
    "        =======================================================\n",
    "        \"\"\"\n",
    "        h = embed\n",
    "        \n",
    "        for i in range(len(weights)-1):\n",
    "            h = tf.matmul(h, weights[i]) + biases[i]\n",
    "            h = tf.pow(h, 3)\n",
    "#             h = tf.sigmoid(h)\n",
    "#             h = tf.tanh(h)\n",
    "#             h = tf.nn.relu(h)\n",
    "        return tf.matmul(h, weights[-1])\n",
    "\n",
    "\n",
    "\n",
    "##         Uncomment this for parallel hidden layers\n",
    "#         word = embed[::, :18*Config.embedding_size]\n",
    "#         pos = embed[::, 18*Config.embedding_size: 36*Config.embedding_size]\n",
    "#         dep = embed[::, 36*Config.embedding_size:]\n",
    "#         h_word = tf.pow(tf.matmul(word, weights[0]) + biases[0], 3)\n",
    "#         h_pos = tf.pow(tf.matmul(pos, weights[1]) + biases[1], 3)\n",
    "#         h_dep = tf.pow(tf.matmul(dep, weights[2]) + biases[2], 3)\n",
    "        \n",
    "#         h = tf.concat([h_word, h_pos, h_dep], axis=1)\n",
    "#         return tf.matmul(h, weights[-1])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Config' from 'Config.pyc'>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initailized\n",
      "Average loss at step  0 :  17.084135055541992\n",
      "Average loss at step  100 :  0.85830023214221\n",
      "Average loss at step  200 :  0.2614855660498142\n",
      "Average loss at step  300 :  0.20143190026283264\n",
      "Average loss at step  400 :  0.17788457304239272\n",
      "Average loss at step  500 :  0.15933041274547577\n",
      "\n",
      "Testing on dev set at step  500\n",
      "UAS: 83.4234863026\n",
      "UASnoPunc: 85.1042785282\n",
      "LAS: 80.6690430491\n",
      "LASnoPunc: 82.0296162324\n",
      "\n",
      "UEM: 26.0\n",
      "UEMnoPunc: 27.8235294118\n",
      "ROOT: 83.0\n",
      "\n",
      "Train Finished.\n",
      "Starting to predict on test set\n"
     ]
    }
   ],
   "source": [
    "# Build the graph model\n",
    "graph = tf.Graph()\n",
    "model = DependencyParserModel(graph, embedding_array, Config)\n",
    "\n",
    "num_steps = Config.max_iter\n",
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    model.train(sess, num_steps)\n",
    "\n",
    "    model.evaluate(sess, testSents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
